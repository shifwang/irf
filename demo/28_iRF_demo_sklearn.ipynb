{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo of the scikit-learn fork iRF\n",
    "\n",
    "* The following is a demo of the scikit learn iRF code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Typical Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the required dependencies\n",
    "\n",
    "* In particular `irf_utils` and `irf_jupyter_utils`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ericxia/anaconda/lib/python3.6/site-packages/sklearn/externals/six.py:31: DeprecationWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
      "  \"(https://pypi.org/project/six/).\", DeprecationWarning)\n",
      "/Users/ericxia/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "\n",
    "# Needed for the scikit-learn wrapper function\n",
    "import irf\n",
    "from irf import irf_utils\n",
    "from irf import irf_jupyter_utils\n",
    "from irf.ensemble.forest import RandomForestClassifier\n",
    "from math import ceil\n",
    "\n",
    "# Import our custom utilities\n",
    "from imp import reload\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Fit the Initial Random Forest\n",
    "\n",
    "* Just fit every feature with equal weights per the usual random forest code e.g. DecisionForestClassifier in scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_breast_cancer = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, rf = irf_jupyter_utils.generate_rf_example(n_estimators=20, \n",
    "                                                                             feature_weight=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check out the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training feature dimensions\", X_train.shape, sep = \":\\n\")\n",
    "print(\"\\n\")\n",
    "print(\"Training outcome dimensions\", y_train.shape, sep = \":\\n\")\n",
    "print(\"\\n\")\n",
    "print(\"Test feature dimensions\", X_test.shape, sep = \":\\n\")\n",
    "print(\"\\n\")\n",
    "print(\"Test outcome dimensions\", y_test.shape, sep = \":\\n\")\n",
    "print(\"\\n\")\n",
    "print(\"first 2 rows of the training set features\", X_train[:2], sep = \":\\n\")\n",
    "print(\"\\n\")\n",
    "print(\"first 2 rows of the training set outcomes\", y_train[:2], sep = \":\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Get all Random Forest and Decision Tree Data\n",
    "\n",
    "* Extract in a single dictionary the random forest data and for all of it's decision trees\n",
    "* This is as required for RIT purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_rf_tree_data = irf_utils.get_rf_tree_data(\n",
    "    rf=rf, X_train=X_train, X_test=X_test, y_test=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 3: Get the RIT data and produce RITs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(12)\n",
    "all_rit_tree_data = irf_utils.get_rit_tree_data(\n",
    "    all_rf_tree_data=all_rf_tree_data,\n",
    "    bin_class_type=1,\n",
    "    M=100,\n",
    "    max_depth=2,\n",
    "    noisy_split=False,\n",
    "    num_splits=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform Manual CHECKS on the `irf_utils`\n",
    "* These should be converted to unit tests and checked with `nosetests -v test_irf_utils.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Plot some Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List Ranked Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "feature_importances_rank_idx = all_rf_tree_data['feature_importances_rank_idx']\n",
    "feature_importances = all_rf_tree_data['feature_importances']\n",
    "\n",
    "for f in range(X_train.shape[1]):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1\n",
    "                                   , feature_importances_rank_idx[f]\n",
    "                                   , feature_importances[feature_importances_rank_idx[f]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Ranked Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the feature importances of the forest\n",
    "feature_importances_std = all_rf_tree_data['feature_importances_std']\n",
    "\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(X_train.shape[1])\n",
    "        , feature_importances[feature_importances_rank_idx]\n",
    "        , color=\"r\"\n",
    "        , yerr = feature_importances_std[feature_importances_rank_idx], align=\"center\")\n",
    "plt.xticks(range(X_train.shape[1]), feature_importances_rank_idx)\n",
    "plt.xlim([-1, X_train.shape[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree 0 (First) - Get output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the output against the decision tree graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now plot the trees individually\n",
    "irf_jupyter_utils.draw_tree(decision_tree = all_rf_tree_data['rf_obj'].estimators_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare to our dict of extracted data from the tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#irf_jupyter_utils.pretty_print_dict(inp_dict = all_rf_tree_data['dtree0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Count the number of samples passing through the leaf nodes\n",
    "sum(all_rf_tree_data['dtree0']['tot_leaf_node_values'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check output against the diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#irf_jupyter_utils.pretty_print_dict(inp_dict = all_rf_tree_data['dtree0']['all_leaf_paths_features'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the iRF function\n",
    "\n",
    "We will run the iRF with the following **parameters**\n",
    "\n",
    "#### Data:\n",
    "* breast cancer binary classification data\n",
    "* **random state (for reproducibility):** 2018\n",
    "\n",
    "#### Weighted RFs\n",
    "* **K:** 5 iterations\n",
    "* **number of trees:** 20\n",
    "\n",
    "#### Bootstrap RFs\n",
    "* **proportion of bootstrap samples:** 20%\n",
    "* **B:** 30 bootstrap samples\n",
    "* **number of trees (bootstrap RFs):** 5 iterations\n",
    "\n",
    "#### RITs (on the bootstrap RFs)\n",
    "* **M:** 20 RITs per forest\n",
    "* **filter label type:** 1-class only\n",
    "* **Max Depth:** 5\n",
    "* **Noisy Split:** False\n",
    "* **Number of splits at Node:** 2 splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the iRF is easy - single function call\n",
    "\n",
    "* All of the bootstrap, RIT complexity is covered through the key parameters passed through\n",
    "in the main algorithm (as listed above)\n",
    "* This function call returns the following data:\n",
    "    1. all RF weights\n",
    "    2. all the K RFs that are iterated over\n",
    "    3. all of the B bootstrap RFs that are run\n",
    "    4. all the B*M RITs that are run on the bootstrap RFs\n",
    "    5. the stability score\n",
    "    \n",
    "### This is a lot of data returned!\n",
    "\n",
    "Will be useful when we build the **interface** later\n",
    "\n",
    "### Let's run it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_rf_weights, all_K_iter_rf_data, \\\n",
    "all_rf_bootstrap_output, all_rit_bootstrap_output, \\\n",
    "stability_score = irf_utils.run_iRF(X_train=X_train,\n",
    "                                    X_test=X_test,\n",
    "                                    y_train=y_train,\n",
    "                                    y_test=y_test,\n",
    "                                    K=5,\n",
    "                                    n_estimators=20,\n",
    "                                    B=30,\n",
    "                                    random_state_classifier=2018,\n",
    "                                    propn_n_samples=.2,\n",
    "                                    bin_class_type=1,\n",
    "                                    M=20,\n",
    "                                    max_depth=5,\n",
    "                                    noisy_split=False,\n",
    "                                    num_splits=2,\n",
    "                                    n_estimators_bootstrap=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stability_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine the stability scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "irf_jupyter_utils._get_histogram(stability_score, sort = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's interesting - feature 22, 27, 20, 23 keep popping up!\n",
    "\n",
    "We should probably look at the feature importances to understand if there is a useful correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine feature importances\n",
    "In particular, let us see how they change over the K iterations of random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for k in range(5): \n",
    "    \n",
    "    iteration = \"rf_iter{}\".format(k)\n",
    "    \n",
    "    feature_importances_std = all_K_iter_rf_data[iteration]['feature_importances_std']\n",
    "    feature_importances_rank_idx = all_K_iter_rf_data[iteration]['feature_importances_rank_idx']\n",
    "    feature_importances = all_K_iter_rf_data[iteration]['feature_importances']\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    title = \"Feature importances; iteration = {}\".format(k)\n",
    "    plt.title(title)\n",
    "    plt.bar(range(X_train.shape[1])\n",
    "            , feature_importances[feature_importances_rank_idx]\n",
    "            , color=\"r\"\n",
    "            , yerr = feature_importances_std[feature_importances_rank_idx], align=\"center\")\n",
    "    plt.xticks(range(X_train.shape[1]), feature_importances_rank_idx, rotation='vertical')\n",
    "    plt.xlim([-1, X_train.shape[1]])\n",
    "    plt.show()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Observations\n",
    "* Note that after 5 iterations, the most important features were found to be 22, 27, 7, and 23\n",
    "* Now also recall that the most stable interactions were found to be '22_27', '7_22', '7_22_27', '23_27', '7_27', '22_23_27'\n",
    "* Given the overlap between these two plots, the results are not unreasonable here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore iRF Data Further\n",
    "\n",
    "## We can look at the decision paths of the Kth RF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the final iteration RF - the key validation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "irf_jupyter_utils.pretty_print_dict(all_K_iter_rf_data['rf_iter4']['rf_validation_metrics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now plot the trees individually\n",
    "irf_jupyter_utils.draw_tree(decision_tree = all_K_iter_rf_data['rf_iter4']['rf_obj'].estimators_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can get this data quite easily in a convenient format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "irf_jupyter_utils.pretty_print_dict(\n",
    "    all_K_iter_rf_data['rf_iter4']['dtree0']['all_leaf_paths_features'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This checks nicely against the plotted diagram above.\n",
    "\n",
    "In fact - we can go further and plot some interesting data from the Decision Trees\n",
    "- This can help us understand variable interactions better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "irf_jupyter_utils.pretty_print_dict(\n",
    "    all_K_iter_rf_data['rf_iter4']['dtree0']['all_leaf_node_values'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can also look at the frequency that a feature appears along a decision path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "irf_jupyter_utils._hist_features(all_K_iter_rf_data['rf_iter4'], n_estimators = 20, \\\n",
    "                         title = 'Frequency of features along decision paths : iteration = 4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common features that appeared were 27,22,23, and 7. This matches well with the feature importance plot above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run some Sanity Checks\n",
    "\n",
    "## Run iRF for just 1 iteration - should be the uniform sampling version\n",
    "\n",
    "This is just a sanity check: the feature importances from iRF after 1 iteration should match the feature importance from running a standard RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_K_iter_rf_data.keys()\n",
    "print(all_K_iter_rf_data['rf_iter0']['feature_importances'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare to the original single fitted random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=20, random_state=2018)\n",
    "rf.fit(X=X_train, y=y_train)\n",
    "print(rf.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And they match perfectly as expected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_rf_weights['rf_weight1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_K_iter_rf_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_rf_bootstrap_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_rit_bootstrap_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stability_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End Wrapper test"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
